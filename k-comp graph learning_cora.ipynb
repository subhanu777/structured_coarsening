{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53a1e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as skd\n",
    "import sklearn.metrics as skm\n",
    "from time import *\n",
    "from tqdm import tqdm\n",
    "def A_to_L(A):\n",
    "    D=np.diag(np.sum(A,axis=0))\n",
    "    return D-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf29680f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#First, we use the spectralGraphTopology to denoise laplacian matrices polluted with noise. \n",
    "# We generate a 4 components adjacency matrix\n",
    "\n",
    "\n",
    "\n",
    "n_class_feats = 5\n",
    "n_classes = 4\n",
    "n_feats = n_classes * n_class_feats\n",
    "prob_intra = 1\n",
    "prob_extra = 0.3\n",
    "max_weight_intra = 1\n",
    "max_weight_extra = 0.3\n",
    "adj = np.zeros((n_feats, n_feats))\n",
    "for i in range(n_classes):\n",
    "    i_start = i * n_class_feats\n",
    "    i_end = i_start + n_class_feats\n",
    "    for ii in range(i_start, i_end):\n",
    "        for jj in range(ii+1, i_end):\n",
    "            adj[ii, jj] = np.random.binomial(n=1, p=prob_intra) * np.random.uniform(high=max_weight_intra)\n",
    "plt.matshow(adj+adj.T)\n",
    "plt.title(\"Original matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b044de2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(n_feats):\n",
    "    for j in range(i+1, n_feats):\n",
    "        adj[i, j] += np.random.binomial(n=1, p=prob_extra) * np.random.uniform(high=max_weight_extra)\n",
    "adj = adj + adj.T\n",
    "plt.matshow(adj)\n",
    "plt.title(\"Noisy matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5518072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helpers\n",
    "import numpy as np\n",
    "from sklearn.isotonic import *\n",
    "from numba import njit\n",
    "\n",
    "# Computes the Adjacency linear operator which maps a vector of weights into\n",
    "# a valid Adjacency matrix.\n",
    "# @param w weight vector of the graph\n",
    "# @return Aw the Adjacency matrix\n",
    "def Ad(v):#TODO check new version still works\n",
    "    \"\"\"take an p(p-1)//2 array and return the adjacency matrice\"\"\"\n",
    "    p=1\n",
    "    while (p*(p-1))//2!=v.shape[0]:\n",
    "        p+=1\n",
    "    a = np.zeros([p,p])\n",
    "    s=0\n",
    "    for nb in range(p-1,0,-1):\n",
    "        i=p-1-nb\n",
    "        a[i][i+1:]=v[s:s+p-i-1]\n",
    "        \"\"\"for j in range(i+1,p):\n",
    "            a[i][j] = v[s+j-i-1]\n",
    "            a[j][i] = v[s+j-i-1]\"\"\"\n",
    "        s += nb\n",
    "    a+=a.T\n",
    "    return a\n",
    "\n",
    "# Computes the Laplacian linear operator which maps a vector of weights into a valid Laplacian matrix.\n",
    "# @param w weight vector of the graph\n",
    "# @return Lw the Laplacian matrix\n",
    "def La(v):\n",
    "    a = -Ad(v)\n",
    "    for k in range(a.shape[0]):\n",
    "        a[k][k]=-np.sum(a[k])\n",
    "    return a\n",
    "\n",
    "@njit\n",
    "def Lstar(M):\n",
    "  \"\"\"\n",
    "  Compute the adjoint operator of L\n",
    "  \"\"\"\n",
    "  N = M.shape[1]\n",
    "  k = (N * (N - 1)) // 2\n",
    "  j, l = 0, 1\n",
    "  w = np.zeros(k)\n",
    "  for i in np.arange(k):\n",
    "    w[i] = M[j, j] + M[l, l] - (M[l, j] + M[j, l])\n",
    "    if (l == (N - 1)):\n",
    "        j += 1\n",
    "        l = j + 1\n",
    "    else:\n",
    "      l += 1\n",
    "  return w\n",
    "\n",
    "#Computes the matrix form of the composition of the operators Lstar and\n",
    "# L, i.e., Lstar o L.\n",
    "#\n",
    "# @param n number of columns/rows\n",
    "# @return M the composition of Lstar and L\n",
    "def Mmat(n):\n",
    "  e = np.zeros(n)\n",
    "  M = np.zeros([n, n])\n",
    "  e[0] = 1\n",
    "  M[0] = Lstar(La(e))\n",
    "  for j in np.arange(1,n):\n",
    "    e[j - 1] = 0\n",
    "    e[j] = 1\n",
    "    M[j] = Lstar(L(e))\n",
    "  return M.T\n",
    "\n",
    "@njit\n",
    "def Astar(M):\n",
    "  N = M.shape[1]\n",
    "  k = (N * (N - 1))//2\n",
    "  j = 0\n",
    "  l = 1\n",
    "  w=np.zeros(k)\n",
    "\n",
    "  for i in np.arange(k):\n",
    "    w[i] = M[l, j] + M[j, l]\n",
    "    if l == (N - 1):\n",
    "      j+=1\n",
    "      l = j+1\n",
    "    else:\n",
    "      l+=1\n",
    "  return w\n",
    "\n",
    "\n",
    "# Computes the matrix form of the composition of the operators Astar and\n",
    "# A, i.e., Astar o A.\n",
    "def Pmat(n):\n",
    "  e = np.zeros(n)\n",
    "  M = np.zeros([n, n])\n",
    "  e[0] = 1;\n",
    "  M[0] = Astar(Ad(e))\n",
    "  for j in np.arange(1,n):\n",
    "    e[j - 1] = 0\n",
    "    e[j] = 1\n",
    "    M[j] = Astar(A(e))\n",
    "  return M.T\n",
    "\n",
    "def vec(M):\n",
    "  return M.T.flatten()\n",
    "\n",
    "def vecLmat(n):\n",
    "  ncols = (n * (n - 1))//2\n",
    "  nrows = n * n\n",
    "\n",
    "  e = np.zeros(ncols)\n",
    "  R = np.zeros([nrows,ncols])\n",
    "  e[0] = 1;\n",
    "  R[0] = vec(L(e));\n",
    "  for j in np.arange(1,ncols):\n",
    "    e[j - 1] = 0;\n",
    "    e[j] = 1;\n",
    "    R[j] = vec(L(e));\n",
    "  return R.T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Computes the inverse of the L operator.\n",
    "\n",
    "# @param M Laplacian matrix\n",
    "# @return w the weight vector of the graph\n",
    "def Linv(M):\n",
    "  n = M.shape[0]\n",
    "  return np.concatenate([-M[i][i+1:] for i in np.arange(n)])\n",
    "\n",
    "#get the n(n-1)//2 vector from the laplacian(or A?)\n",
    "#M is laplacian\n",
    "#w is weight vector\n",
    "def Ainv(M):\n",
    "  n = M.shape[0]\n",
    "  return np.concatenate([M[i][i+1:] for i in np.arange(n)])\n",
    "\n",
    "\n",
    "def isoreg(y):\n",
    "    \"\"\"\n",
    "    Compute the isotonic regression of a vector y\n",
    "    For compatibity reasons, we chosse an arbitrary x as training data\n",
    "    but x is useless as we only keep the estimates of the y_i and never interpolate\n",
    "    \"\"\"\n",
    "    x = np.arange(len(y))\n",
    "    isoreg = IsotonicRegression()\n",
    "    isoreg.fit(x, y)\n",
    "    return isoreg.f_(x)\n",
    "\n",
    "\n",
    "\n",
    "def w_init(w0, Sinv):\n",
    "  \"\"\"\n",
    "  Initialize w0, the vectorized upper triangular coefficients of the adjacency matrix\n",
    "  \"\"\"\n",
    "  if type(w0) is str:\n",
    "    if (w0 == \"qp\"):\n",
    "      R = vecLmat(Sinv.shape[1])\n",
    "      qp = 0\n",
    "      assert False,\"idk\"\n",
    "      #quadprog::solve.QP(crossprod(R), t(R) %*% vec(Sinv), diag(ncol(R)))\n",
    "      w0 = qp#qp$solution\n",
    "    elif (w0 == \"naive\"):\n",
    "      w0 = Linv(Sinv)\n",
    "      w0[w0 < 0] = 0 # Should not happen\n",
    "  return w0\n",
    "\n",
    "\n",
    "\n",
    "def laplacian_w_update(w, Lw, U, beta, lambd, K, p):\n",
    "  \"\"\"\n",
    "  Update w according to equation 38\n",
    "  \"\"\"\n",
    "  t = lambd[:, None]**0.5 * U.T\n",
    "  c = Lstar(t.T@t - K / beta)\n",
    "  grad_f = Lstar(Lw) - c\n",
    "  if 1:\n",
    "    M_grad_f = - Lstar(La(grad_f))\n",
    "    wT_M_grad_f = np.sum(w * M_grad_f)\n",
    "    dwT_M_dw = np.sum(grad_f * M_grad_f)\n",
    "  # exact line search\n",
    "    t = (wT_M_grad_f - np.sum(c * grad_f)) / dwT_M_dw\n",
    "  else:\n",
    "      t=1/(2*p)\n",
    "  w_update = w - t * grad_f\n",
    "  w_update[w_update < 0] = 0\n",
    "  return w_update\n",
    "\n",
    "\n",
    "\n",
    "def joint_w_update(w, Lw, Aw, U, V, lambd, psi, beta, nu, K):\n",
    "  t=lambd[:, None]**0.5*U.T\n",
    "  ULmdUT = t.T@t\n",
    "  VPsiVT = V @ np.diag(psi) @ V.T\n",
    "  c1 = Lstar(beta * ULmdUT - K)\n",
    "  c2 = nu * Astar(VPsiVT)\n",
    "  Mw = Lstar(Lw)\n",
    "  Pw = 2 * w\n",
    "  grad_f1 = beta * Mw - c1\n",
    "  M_grad_f1 = Lstar(La(grad_f1))\n",
    "  grad_f2 = nu * Pw - c2\n",
    "  P_grad_f2 = 2 * grad_f2\n",
    "  grad_f = grad_f1 + grad_f2\n",
    "  t = np.sum((beta * Mw + nu * Pw - (c1 + c2)) * grad_f) / np.sum(grad_f * (beta * M_grad_f1 + nu * P_grad_f2))\n",
    "  w_update = w - t * (grad_f1 + grad_f2)\n",
    "  w_update[w_update < 0] = 0\n",
    "  return w_update\n",
    "\n",
    "\n",
    "def bipartite_w_update(w, Aw, V, nu, psi, K, J, Lips):\n",
    "  reg_eps = 0\n",
    "  grad_h = 2 * w - Astar(V @ np.diag(psi) @ V.T) #+ Lstar(K) / beta#\n",
    "  w_update = w - (Lstar(np.linalg.inv(La(w) + J+np.eye(J.shape[0])*reg_eps) + K) + nu * grad_h) / (2 * nu + Lips)\n",
    "  w_update[w_update < 0] = 0#TODO faire en sorte que la régularisation ligne précédent ne soit pas nécessaire\n",
    "  return w_update\n",
    "\n",
    "\n",
    "\n",
    "def laplacian_U_update(Lw, k):\n",
    "  \"\"\"\n",
    "  Return all but the k first eigenvectors of the Laplacian Lw\n",
    "  \"\"\"\n",
    "  return np.linalg.eigh(Lw)[1][:, k:]\n",
    "\n",
    "\n",
    "def bipartite_V_update(Aw, z):\n",
    "  n = Aw.shape[1]\n",
    "  V = np.linalg.eigh(Aw)[1]\n",
    "  return np.concatenate([V[:, :(n - z)//2], V[:,(n + z)//2:n]],axis=1)\n",
    "\n",
    "\n",
    "def joint_U_update(Lw,k):\n",
    "  return np.linalg.eigh(Lw)[1][:, k:]\n",
    "\n",
    "\n",
    "def joint_V_update(Aw,z):\n",
    "  return bipartite_V_update(Aw,z)\n",
    "\n",
    "\n",
    "\n",
    "def laplacian_lambda_update(lb, ub, beta, U, Lw, k):\n",
    "  \"\"\"\n",
    "  Update lambda according to algorithm 1\n",
    "  \"\"\"\n",
    "  q = Lw.shape[1] - k\n",
    "  d = np.diagonal(U.T @ Lw @ U)\n",
    "  # unconstrained solution as initial point\n",
    "  lambd = .5 * (d + (d**2 + 4 / beta)**0.5)\n",
    "  eps = 1e-9\n",
    "  condition_ub = np.array([(lambd[q-1] - ub) <= eps])\n",
    "  condition_lb = np.array([(lambd[0] - lb) >= -eps])\n",
    "  condition_ordered = (lambd[1:q] - lambd[0:(q-1)]) >= -eps\n",
    "  condition = np.concatenate([condition_ub,\\\n",
    "                 condition_lb,\\\n",
    "                 condition_ordered])\n",
    "  if np.all(condition):\n",
    "    return lambd\n",
    "  else:\n",
    "    greater_ub = lambd > ub\n",
    "    lesser_lb = lambd < lb\n",
    "    lambd[greater_ub] = ub\n",
    "    lambd[lesser_lb] = lb\n",
    "  condition_ub = np.array([(lambd[q-1] - ub) <= eps])\n",
    "  condition_lb = np.array([(lambd[0] - lb) >= -eps])\n",
    "  condition_ordered = (lambd[1:q] - lambd[:(q-1)]) >= -eps\n",
    "  condition = np.concatenate([condition_ub,\\\n",
    "                 condition_lb,\\\n",
    "                 condition_ordered])\n",
    "  if np.all(condition):\n",
    "    return (lambd)\n",
    "  else:\n",
    "    print(lambd)\n",
    "    raise ValueError('eigenvalues are not in increasing order consider increasing the value of beta')\n",
    "\n",
    "\n",
    "def bipartite_psi_update(V, Aw, lb = -np.inf, ub = np.inf):\n",
    "  c = np.diagonal(V.T @ Aw @ V)\n",
    "  n = c.shape[0]\n",
    "  c_tilde = .5 * (c[(n//2):][::-1] - c[:(n//2)])\n",
    "  x = isoreg(c_tilde[::-1])\n",
    "  #x <- stats::isoreg(rev(c_tilde))$yf # R\n",
    "  x = np.concatenate((-x[::-1], x))\n",
    "  #x <- c(-rev(x), x) # R\n",
    "  x[x < lb] = lb\n",
    "  x[x > ub] = ub\n",
    "  return x\n",
    "\n",
    "\n",
    "def Ad(v):#TODO check new version still works\n",
    "    \"\"\"take an p(p-1)//2 array and return the adjacency matrice\"\"\"\n",
    "    p=1\n",
    "    while (p*(p-1))//2!=v.shape[0]:\n",
    "        p+=1\n",
    "    a = np.zeros([p,p])\n",
    "    s=0\n",
    "    for nb in range(p-1,0,-1):\n",
    "        i=p-1-nb\n",
    "        a[i][i+1:]=v[s:s+p-i-1]\n",
    "        \"\"\"for j in range(i+1,p):\n",
    "            a[i][j] = v[s+j-i-1]\n",
    "            a[j][i] = v[s+j-i-1]\"\"\"\n",
    "        s += nb\n",
    "    a+=a.T\n",
    "    return a\n",
    "\n",
    "\n",
    "def La(v):\n",
    "    a = -Ad(v)\n",
    "    for k in range(a.shape[0]):\n",
    "        a[k][k]=-np.sum(a[k])\n",
    "    return a\n",
    "\n",
    "\n",
    "def Linv(M):\n",
    "  n = M.shape[0]\n",
    "  return np.concatenate([-M[i][i+1:] for i in np.arange(n)])\n",
    "\n",
    "#get the n(n-1)//2 vector from the laplacian(or A?)\n",
    "#M is laplacian\n",
    "#w is weight vector\n",
    "def Ainv(M):\n",
    "  n = M.shape[0]\n",
    "  return np.concatenate([M[i][i+1:] for i in np.arange(n)])\n",
    "\n",
    "\n",
    "\n",
    "def w_init(w0, Sinv):\n",
    "  \"\"\"\n",
    "  Initialize w0, the vectorized upper triangular coefficients of the adjacency matrix\n",
    "  \"\"\"\n",
    "  if type(w0) is str:\n",
    "    if (w0 == \"qp\"):\n",
    "      R = vecLmat(Sinv.shape[1])\n",
    "      qp = 0\n",
    "      assert False,\"idk\"\n",
    "      #quadprog::solve.QP(crossprod(R), t(R) %*% vec(Sinv), diag(ncol(R)))\n",
    "      w0 = qp#qp$solution\n",
    "    elif (w0 == \"naive\"):\n",
    "      w0 = Linv(Sinv)\n",
    "      w0[w0 < 0] = 0 # Should not happen\n",
    "  return w0\n",
    "\n",
    "\n",
    "def pairwise_matrix_rownorm(M):\n",
    "    \"\"\"\n",
    "    Compute the matrix E where Eij is ||x_i - x_j||**2\n",
    "    \"\"\"\n",
    "    n = M.shape[0]\n",
    "    V = np.zeros([n, n])\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            V[i][j]=np.linalg.norm(M[i]-M[j])**2\n",
    "    V+=V.T\n",
    "    return V\n",
    "def build_initial_graph(Y, m):\n",
    "    # if well understood create the m nearest neighboor directed graph\n",
    "    n = Y.shape[0]\n",
    "    A = np.zeros([n, n])\n",
    "    E = pairwise_matrix_rownorm(Y)\n",
    "    for i in np.arange(0, n):\n",
    "        sorted_index = np.argsort(E[i])\n",
    "        j_sweep = sorted_index[1:m+1]\n",
    "        den = m * E[i][sorted_index[m+1]] - np.sum(E[i][j_sweep]) # renormalization, but why is it like that?\n",
    "        ei = E[i, sorted_index[m+1]]\n",
    "        for j in j_sweep:\n",
    "            A[i,j] = (ei - E[i, j]) / den\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655ebfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_k_component_graph (S, is_data_matrix = False, k = 1, w0 = \"naive\", lb = 0, ub = 1e4, alpha = 0,\\\n",
    "                                    beta = 1e4, beta_max = 1e6, fix_beta = True, rho = 1e-2, m = 7,\\\n",
    "                                    maxiter = 1e4, abstol = 1e-6, reltol = 1e-4, eigtol = 1e-9,\\\n",
    "                                    record_objective = False, record_weights = False, verbose = True):\n",
    "  \"\"\"\n",
    "  Learn the Laplacian and adjacency matrix corresponding to a k-component graph\n",
    "  Params:\n",
    "    S: Either the original correlation matrix or the raw data matrix\n",
    "    is_data_matrix: bool, if True then the correlations matrix is computed from S\n",
    "    k: number of components of the final graph\n",
    "    m: number of neighbors considered to build the matrix (only useful if is_data_matrix is true)\n",
    "  \"\"\"\n",
    "  if (is_data_matrix or S.shape[0] != S.shape[1]):\n",
    "    A = build_initial_graph(S, m = m)\n",
    "    D = np.diag(.5 * (np.sum(A,axis=1) + np.sum(A,axis=0)))\n",
    "    L = D - .5 * (A + A.T)\n",
    "    S = np.linalg.pinv(L)\n",
    "    is_data_matrix = True\n",
    "  # number of nodes\n",
    "  n = S.shape[0]\n",
    "  # l1-norm penalty factor\n",
    "  H = alpha * (2 *np.eye(n) - np.ones([n, n]))\n",
    "  K = S + H\n",
    "  # find an appropriate inital guess\n",
    "  if (is_data_matrix):\n",
    "    Sinv = L\n",
    "  else:\n",
    "    Sinv = np.linalg.pinv(S)\n",
    "  # if w0 is either \"naive\" or \"qp\", compute it, else return w0\n",
    "  w0 = w_init(w0, Sinv)\n",
    "  # compute quantities on the initial guess\n",
    "  Lw0 = La(w0)\n",
    "  U0 = laplacian_U_update(Lw = Lw0, k = k)\n",
    "  lambda0 = laplacian_lambda_update(lb = lb, ub = ub, beta = beta, U = U0,\\\n",
    "                                     Lw = Lw0, k = k)\n",
    "\n",
    "  beta_seq = [beta]\n",
    "  time_seq = [0]\n",
    "  start_time = time()\n",
    "  for i in tqdm(range(maxiter)):\n",
    "    #test_time = time()\n",
    "    #test_total_time = time()\n",
    "    w = laplacian_w_update(w = w0, Lw = Lw0, U = U0, beta = beta,\\\n",
    "                            lambd = lambda0, K = K, p=S.shape[0])\n",
    "    #test_laplacian_w_update_time= time() - test_time\n",
    "    #test_time = time()\n",
    "    Lw = La(w)\n",
    "    #test_La_time = time() - test_time\n",
    "    #test_time = time()\n",
    "    U = laplacian_U_update(Lw = Lw, k = k)\n",
    "    #test_laplacian_U_update_time = time() - test_time\n",
    "    #test_time = time()\n",
    "    lambd = laplacian_lambda_update(lb = lb, ub = ub, beta = beta, U = U,\\\n",
    "                                      Lw = Lw, k = k)\n",
    "    #test_laplacian_lambda_update_time = time() - test_time\n",
    "    #test_time = time()\n",
    "    # check for convergence\n",
    "    werr = abs(w0 - w)\n",
    "    has_w_converged = (np.all(werr <= .5 * reltol * (w + w0)) or np.all(werr <= abstol))\n",
    "    time_seq.append(time()-start_time)\n",
    "    if not(fix_beta):\n",
    "      eigvals=np.linalg.eigh(Lw)[0]\n",
    "      n_zero_eigenvalues = np.sum(abs(eigvals) < eigtol)\n",
    "      if (k <= n_zero_eigenvalues):\n",
    "        beta = (1 + rho) * beta\n",
    "      elif (k > n_zero_eigenvalues):\n",
    "        beta = beta / (1 + rho)\n",
    "      if (beta > beta_max):\n",
    "        beta = beta_max\n",
    "      beta_seq.append(beta)\n",
    "    if has_w_converged:\n",
    "      break\n",
    "    # update estimates\n",
    "    w0 = w\n",
    "    U0 = U\n",
    "    lambda0 = lambd\n",
    "    Lw0 = Lw\n",
    "    \"\"\"\n",
    "    test_convergence_time = time() - test_time\n",
    "    test_total_time = time() - test_total_time\n",
    "    print('total time', test_total_time)\n",
    "    print('total ratio (1):', (test_laplacian_w_update_time + test_La_time + test_laplacian_U_update_time + test_laplacian_lambda_update_time + test_convergence_time)/test_total_time)\n",
    "    print('laplacian_w_update', test_laplacian_w_update_time/test_total_time*100)\n",
    "    print('La', test_La_time/test_total_time*100)\n",
    "    print('laplacian_U_update', test_laplacian_U_update_time/test_total_time*100)\n",
    "    print('laplacian_lambda_update', test_laplacian_lambda_update_time/test_total_time*100)\n",
    "    print('convergence', test_convergence_time/test_total_time*100)\"\"\"\n",
    "  # compute the adjacency matrix\n",
    "  Aw = Ad(w)\n",
    "  results = {\"Laplacian\" : Lw, \"Adjacency\" : Aw, \"w\" : w, \"lambd\" : lambd, \"U\" : U,\\\n",
    "                 \"elapsed_time\" : time_seq, \"convergence\" : has_w_converged,\\\n",
    "                  \"beta_seq\" : beta_seq}\n",
    "  return results\n",
    "\n",
    "def learn_cospectral_graph(S, lambd, k = 1, is_data_matrix = False, w0 = \"naive\", alpha = 0,\\\n",
    "                                   beta = 1e4, beta_max = 1e6, fix_beta = True, rho = 1e-2, m = 7,\\\n",
    "                                   maxiter = 1e4, abstol = 1e-6, reltol = 1e-4, eigtol = 1e-9,\\\n",
    "                                   record_objective = False, record_weights = False, verbose = True):\n",
    "  if (is_data_matrix or S.shape[0] != S.shape[1]):\n",
    "    A = build_initial_graph(S, m = m)\n",
    "    D = np.diag(.5 * (np.sum(A,axis=1) + np.sum(A,axis=0)))\n",
    "    L = D - .5 * (A + A.T)\n",
    "    S = np.linalg.pinv(L)\n",
    "    is_data_matrix = True\n",
    "  # number of nodes\n",
    "  n = S.shape[0]\n",
    "  # l1-norm penalty factor\n",
    "  H = alpha * (2 * np.eye(n)- np.ones(n, n))\n",
    "  K = S + H\n",
    "  # find an appropriate inital guess\n",
    "  if (is_data_matrix):\n",
    "    Sinv = L\n",
    "  else:\n",
    "    Sinv = np.linalg.pinv(S)\n",
    "  # if w0 is either \"naive\" or \"qp\", compute it, else return w0\n",
    "  w0 = w_init(w0, Sinv)\n",
    "  # compute quantities on the initial guess\n",
    "  Lw0 = La(w0)\n",
    "  U0 = laplacian_U_update(Lw = Lw0, k = k)\n",
    "  beta_seq = [beta]\n",
    "  time_seq = [0]\n",
    "  start_time = time()\n",
    "  for i in np.arange(maxiter):\n",
    "    w = laplacian_w_update(w = w0, Lw = Lw0, U = U0, beta = beta,\\\n",
    "                            lambd = lambd, K = K)\n",
    "    Lw = La(w)\n",
    "    U = laplacian_U_update(Lw = Lw, k = k)\n",
    "    # check for convergence\n",
    "    werr = abs(w0 - w)\n",
    "    has_w_converged = min(werr <= .5 * reltol * (w + w0)) or min(werr <= abstol)\n",
    "    time_seq.append(time() - start_time)\n",
    "    if not(fix_beta):\n",
    "      eigvals = np.linalg.eigh(Lw)[0]\n",
    "      n_zero_eigenvalues = np.sum(abs(eigvals) < eigtol)\n",
    "      if (k <= n_zero_eigenvalues):\n",
    "        beta = (1 + rho) * beta\n",
    "      elif (k > n_zero_eigenvalues):\n",
    "        beta = beta / (1 + rho)\n",
    "      if (beta > beta_max):\n",
    "        beta = beta_max\n",
    "      beta_seq.append(beta)\n",
    "    if (has_w_converged):\n",
    "      break\n",
    "    # update estimates\n",
    "    w0 = w\n",
    "    U0 = U\n",
    "    Lw0 = Lw\n",
    "  # compute the adjacency matrix\n",
    "  Aw = Ad(w)\n",
    "  results = {\"Laplacian\" : Lw, \"Adjacency\" : Aw, \"w\" : w, \"lambd\" : lambd, \"U\" : U,\\\n",
    "                  \"elapsed_time\" : time_seq, \"convergence\" : has_w_converged,\\\n",
    "                  \"beta_seq\" : beta_seq}\n",
    "  return results\n",
    "\n",
    "def learn_bipartite_graph(S, is_data_matrix = False, z = 0, nu = 1e4, alpha = 0.,\n",
    "                                  w0 = \"naive\", m = 7, maxiter = 1e4, abstol = 1e-6, reltol = 1e-4,\n",
    "                                  record_weights = False, verbose = True):\n",
    "  if is_data_matrix or S.shape[0] != S.shape[1]:\n",
    "    A = build_initial_graph(S, m = m)\n",
    "    D =  np.diag(.5 * (np.sum(A,axis=1) + np.sum(A,axis=0)))\n",
    "    L = D - .5 * (A + A.T)\n",
    "    S = np.linalg.pinv(L)\n",
    "    is_data_matrix = True\n",
    "  # number of nodes\n",
    "  n = S.shape[0]\n",
    "  # note now that S is always some sort of similarity matrix\n",
    "  J = np.ones([n,n])/n\n",
    "  # l1-norm penalty factor\n",
    "  H = alpha * (2 * np.eye(n) - np.ones([n, n]))\n",
    "  K = S + H\n",
    "  # compute initial guess\n",
    "  if is_data_matrix:\n",
    "    Sinv = L\n",
    "  else:\n",
    "    Sinv = np.linalg.pinv(S)\n",
    "  # if w0 is either \"naive\" or \"qp\", compute it, else return w0\n",
    "  w0 = w_init(w0, Sinv)\n",
    "  Lips = 1 / np.linalg.eigh(La(w0) + J)[0][0]\n",
    "  # compute quantities on the initial guess\n",
    "  Aw0 = Ad(w0)\n",
    "  V0 = bipartite_V_update(Aw0, z)\n",
    "  psi0 = bipartite_psi_update(V0, Aw0)\n",
    "  Lips_seq = [Lips]\n",
    "  time_seq = [0]\n",
    "  start_time = time()\n",
    "  ll0 = bipartite_likelihood(Lw = La(w0), K = K, J = J)\n",
    "  fun0 = ll0 + bipartite_prior(nu = nu, Aw = Aw0, psi = psi0, V = V0)\n",
    "  fun_seq = [fun0]\n",
    "  ll_seq = [ll0]\n",
    "  for i in np.arange(maxiter):\n",
    "    # we need to make sure that the Lipschitz constant is large enough\n",
    "    # in order to avoid divergence\n",
    "    while 1:\n",
    "      # compute the update for w\n",
    "      w = bipartite_w_update(w = w0, Aw = Aw0, V = V0, nu = nu, psi = psi0,\n",
    "                              K = K, J = J, Lips = Lips)\n",
    "      # compute the objective function at the updated value of w\n",
    "      fun_t=bipartite_obj_fun(Aw = Ad(w), Lw = La(w), V = V0, psi = psi0,\n",
    "                        K = K, J = J, nu = nu)\n",
    "      \"\"\"fun_t = tryCatch({#TODO\n",
    "                   bipartite.obj_fun(Aw = A(w), Lw = L(w), V = V0, psi = psi0,\n",
    "                                     K = K, J = J, nu = nu)\n",
    "                 }, warning = function(warn) return(Inf), error = function(err) return(Inf)\n",
    "               )\"\"\"\n",
    "      # check if the previous value of the objective function is\n",
    "      # smaller than the current one\n",
    "      Lips_seq.append(Lips)\n",
    "      if fun0 < fun_t:\n",
    "        # in case it is in fact larger, then increase Lips and recompute w\n",
    "        Lips = 2 * Lips\n",
    "    else:\n",
    "        # otherwise decrease Lips and get outta here!\n",
    "        Lips = .5 * Lips\n",
    "        if Lips < 1e-12:\n",
    "          Lips = 1e-12\n",
    "        break\n",
    "    Lw = La(w)\n",
    "    Aw = Ad(w)\n",
    "    V = bipartite_V_update(Aw = Aw, z = z)\n",
    "    psi = bipartite_psi_update(V = V, Aw = Aw)\n",
    "    # compute negloglikelihood and objective function values\n",
    "    ll = bipartite_likelihood(Lw = Lw, K = K, J = J)\n",
    "    fun = ll + bipartite_prior(nu = nu, Aw = Aw, psi = psi, V = V)\n",
    "    # save measurements of time and objective functions\n",
    "    time_seq.append(time()- start_time)\n",
    "    ll_seq.append(ll)\n",
    "    fun_seq.append(fun)\n",
    "    # compute the relative error and check the tolerance on the Adjacency\n",
    "    # matrix and on the objective function\n",
    "    # check for convergence\n",
    "    werr = abs(w0 - w)\n",
    "    has_w_converged = (np.all(werr <= .5 * reltol * (w + w0)) or np.all(werr <= abstol))\n",
    "    if (has_w_converged):\n",
    "      break\n",
    "    # update estimates\n",
    "    fun0 = fun\n",
    "    w0 = w\n",
    "    V0 = V\n",
    "    psi0 = psi\n",
    "    Aw0 = Aw\n",
    "  results = {\"Laplacian\" : Lw, \"Adjacency\" : Aw, \"obj_fun\" : fun_seq, \"loglike\" : ll_seq, \"w\" : w,\n",
    "                  \"psi\" : psi, \"V\" : V, \"elapsed_time\" : time_seq, \"Lips\" : Lips,\n",
    "                  \"Lips_seq\" : Lips_seq, \"convergence\" : (i < maxiter), \"nu\" : nu}\n",
    "  return results\n",
    "\n",
    "\n",
    "def learn_bipartite_k_component_graph(S, is_data_matrix = False, z = 0, k = 1,\\\n",
    "                                              w0 = \"naive\", m = 7, alpha = 0., beta = 1e4,\\\n",
    "                                              rho = 1e-2, fix_beta = True, beta_max = 1e6, nu = 1e4,\\\n",
    "                                              lb = 0, ub = 1e4, maxiter = 1e4, abstol = 1e-6,\\\n",
    "                                              reltol = 1e-4, eigtol = 1e-9,\\\n",
    "                                              record_weights = False, record_objective = False, verbose = True):\n",
    "  if is_data_matrix or S.shape[0] != S.shape[1]:\n",
    "    A = build_initial_graph(S, m = m)\n",
    "    D =  np.diag(.5 * (np.sum(A,axis=1) + np.sum(A,axis=0)))\n",
    "    L = D - .5 * (A + A.T)\n",
    "    S = np.linalg.pinv(L)\n",
    "    is_data_matrix = True\n",
    "  # number of nodes\n",
    "  n = S.shape[0]\n",
    "  # note now that S is always some sort of similarity matrix\n",
    "  J = np.ones([n,n])/n\n",
    "  # l1-norm penalty factor\n",
    "  H = alpha * (2 * np.eye(n) - np.ones([n, n]))\n",
    "  K = S + H\n",
    "  # compute initial guess\n",
    "  if is_data_matrix:\n",
    "    Sinv = L\n",
    "  else:\n",
    "    Sinv = np.linalg.pinv(S)\n",
    "  # if w0 is either \"naive\" or \"qp\", compute it, else return w0\n",
    "  w0 = w_init(w0, Sinv)\n",
    "  # compute quantities on the initial guess\n",
    "  Aw0 = Ad(w0)\n",
    "  Lw0 = La(w0)\n",
    "  V0 = joint_V_update(Aw0, z)\n",
    "  psi0 = bipartite_psi_update(V0, Aw0)\n",
    "  U0 = joint_U_update(Lw0, k)\n",
    "  lambda0 = laplacian_lambda_update(lb, ub, beta, U0, Lw0, k)\n",
    "  beta_seq = [beta]\n",
    "  time_seq = [0]\n",
    "  start_time = time()\n",
    "  for i in np.arange(maxiter):\n",
    "    w = joint_w_update(w0, Lw0, Aw0, U0, V0, lambda0, psi0, beta, nu, K)\n",
    "    Lw = La(w)\n",
    "    Aw = Ad(w)\n",
    "    U = joint_U_update(Lw, k)\n",
    "    V = joint_V_update(Aw, z)\n",
    "    lambd = laplacian_lambda_update(lb, ub, beta, U, Lw, k)\n",
    "    psi = bipartite_psi_update(V, Aw)\n",
    "    time_seq.append(time()-start_time)\n",
    "    werr = abs(w0 - w)\n",
    "    has_w_converged = (np.all(werr <= .5 * reltol * (w + w0)) or np.all(werr <= abstol))\n",
    "    time_seq.append(time()-start_time)\n",
    "    eigvals = np.linalg.eigh(Lw)[0]\n",
    "    if not(fix_beta):\n",
    "      n_zero_eigenvalues = sum(abs(eigvals) < eigtol)\n",
    "      if (k < n_zero_eigenvalues):\n",
    "        beta = (1 + rho) * beta\n",
    "      elif (k > n_zero_eigenvalues):\n",
    "        beta = beta / (1 + rho)\n",
    "      if (beta > beta_max):\n",
    "        beta = beta_max\n",
    "      beta_seq.append(beta)\n",
    "    if (has_w_converged):\n",
    "      break\n",
    "    # update estimates\n",
    "    w0 = w\n",
    "    U0 = U\n",
    "    V0 = V\n",
    "    lambda0 = lambd\n",
    "    psi0 = psi\n",
    "    Lw0 = Lw\n",
    "    Aw0 = Aw\n",
    "  results = {\"Laplacian\" : Lw, \"Adjacency\" : Aw, \"w\" : w, \"psi\" : psi,\n",
    "                  \"lambd\" : lambd, \"V\" : V, \"U\" : U, \"elapsed_time\" : time_seq,\n",
    "                  \"beta_seq\" : beta_seq, \"convergence\" : has_w_converged}\n",
    "  return(results)\n",
    "\n",
    "def nb_connected_component(L):\n",
    "    return np.sum(np.linalg.eigh(L)[0]<10**-12)\n",
    "\n",
    "def is_bipartite(A):\n",
    "    n=A.shape[0]\n",
    "    co=[-1]*n\n",
    "    def parc(u):\n",
    "        for v in range(n):\n",
    "            if A[u][v]>0:\n",
    "                if co[v]==-1:\n",
    "                    co[v]=1-co[u]\n",
    "                    if not(parc(v)):\n",
    "                        return False\n",
    "                elif co[v]+co[u]!=1:\n",
    "                    return False\n",
    "        return True\n",
    "    for u in range(n):\n",
    "        if co[u]==-1:\n",
    "            co[u]=0\n",
    "            if not(parc(u)):\n",
    "                return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8e9fce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2708, 1433]) torch.Size([2708, 2708])\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import to_dense_adj\n",
    "import os\n",
    "import torch\n",
    "os.getcwd()\n",
    "dataset = os.path.join(os.getcwd(),'Cora')\n",
    "dataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "dataset= Planetoid(root=dataset, name='Cora')\n",
    "x = dataset[0].x.detach().cpu().numpy()\n",
    "labels = dataset[0].y\n",
    "NO_OF_CLASSES =  len(set(np.array(dataset[0].y)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "adj = to_dense_adj(dataset[0].edge_index)\n",
    "adj = adj[0]\n",
    "labels = dataset[0].y\n",
    "labels = labels.numpy()\n",
    "\n",
    "X = dataset[0].x\n",
    "X = X.to_dense()\n",
    "N = X.shape[0]\n",
    "NO_OF_CLASSES =  len(set(np.array(dataset[0].y)))\n",
    "\n",
    "print(X.shape, adj.shape)\n",
    "\n",
    "nn = int(1*N)\n",
    "X = X[:nn,:]\n",
    "adj = adj[:nn,:nn]\n",
    "A = adj[:nn,:nn]\n",
    "AT= torch.transpose(A,0,1)\n",
    "labels = labels[:nn]\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7b6e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7086/10000 [43:25<17:51,  2.72it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimization converged!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# n_samples = 100 * n_feats   #(2000)\n",
    "# # compute the laplacian and correlation matrices\n",
    "# lap = np.diag(adj.sum(axis=0)) - adj\n",
    "# theta = np.linalg.pinv(lap)\n",
    "# # generate samples\n",
    "# x = np.random.multivariate_normal(np.zeros(n_feats), theta, size=n_samples).T\n",
    "\n",
    "\n",
    "# # learn the laplacian and adjacency matrices\n",
    "res_denoising = learn_k_component_graph(x.T, k=4, maxiter=10000)\n",
    "if res_denoising['convergence']: print(\"The optimization converged!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eda3e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj2=res_denoising['Adjacency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eec24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(X.shape[1], 64)\n",
    "        self.conv2 = GCNConv(64, NO_OF_CLASSES)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    \n",
    "####### NO output layer is written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb048640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9492863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_bipartite_graph(S, is_data_matrix = False, z = 0, nu = 1e4, alpha = 0.,\n",
    "                                  w0 = \"naive\", m = 7, maxiter = 1e4, abstol = 1e-6, reltol = 1e-4,\n",
    "                                  record_weights = False, verbose = True):\n",
    "  if is_data_matrix or S.shape[0] != S.shape[1]:\n",
    "    A = build_initial_graph(S, m = m)\n",
    "    D =  np.diag(.5 * (np.sum(A,axis=1) + np.sum(A,axis=0)))\n",
    "    L = D - .5 * (A + A.T)\n",
    "    S = np.linalg.pinv(L)\n",
    "    is_data_matrix = True\n",
    "  # number of nodes\n",
    "  n = S.shape[0]\n",
    "  # note now that S is always some sort of similarity matrix\n",
    "  J = np.ones([n,n])/n\n",
    "  # l1-norm penalty factor\n",
    "  H = alpha * (2 * np.eye(n) - np.ones([n, n]))\n",
    "  K = S + H\n",
    "  # compute initial guess\n",
    "  if is_data_matrix:\n",
    "    Sinv = L\n",
    "  else:\n",
    "    Sinv = np.linalg.pinv(S)\n",
    "  # if w0 is either \"naive\" or \"qp\", compute it, else return w0\n",
    "  w0 = w_init(w0, Sinv)\n",
    "  Lips = 1 / np.linalg.eigh(La(w0) + J)[0][0]\n",
    "  # compute quantities on the initial guess\n",
    "  Aw0 = Ad(w0)\n",
    "  V0 = bipartite_V_update(Aw0, z)\n",
    "  psi0 = bipartite_psi_update(V0, Aw0)\n",
    "  Lips_seq = [Lips]\n",
    "  time_seq = [0]\n",
    "  start_time = time()\n",
    "  ll0 = bipartite_likelihood(Lw = La(w0), K = K, J = J)\n",
    "  fun0 = ll0 + bipartite_prior(nu = nu, Aw = Aw0, psi = psi0, V = V0)\n",
    "  fun_seq = [fun0]\n",
    "  ll_seq = [ll0]\n",
    "  for i in np.arange(maxiter):\n",
    "    # we need to make sure that the Lipschitz constant is large enough\n",
    "    # in order to avoid divergence\n",
    "    while 1:\n",
    "      # compute the update for w\n",
    "      w = bipartite_w_update(w = w0, Aw = Aw0, V = V0, nu = nu, psi = psi0,\n",
    "                              K = K, J = J, Lips = Lips)\n",
    "      # compute the objective function at the updated value of w\n",
    "      fun_t=bipartite_obj_fun(Aw = Ad(w), Lw = La(w), V = V0, psi = psi0,\n",
    "                        K = K, J = J, nu = nu)\n",
    "      \"\"\"fun_t = tryCatch({#TODO\n",
    "                   bipartite.obj_fun(Aw = A(w), Lw = L(w), V = V0, psi = psi0,\n",
    "                                     K = K, J = J, nu = nu)\n",
    "                 }, warning = function(warn) return(Inf), error = function(err) return(Inf)\n",
    "               )\"\"\"\n",
    "      # check if the previous value of the objective function is\n",
    "      # smaller than the current one\n",
    "      Lips_seq.append(Lips)\n",
    "      if fun0 < fun_t:\n",
    "        # in case it is in fact larger, then increase Lips and recompute w\n",
    "        Lips = 2 * Lips\n",
    "    else:\n",
    "        # otherwise decrease Lips and get outta here!\n",
    "        Lips = .5 * Lips\n",
    "        if Lips < 1e-12:\n",
    "          Lips = 1e-12\n",
    "        break\n",
    "    Lw = La(w)\n",
    "    Aw = Ad(w)\n",
    "    V = bipartite_V_update(Aw = Aw, z = z)\n",
    "    psi = bipartite_psi_update(V = V, Aw = Aw)\n",
    "    # compute negloglikelihood and objective function values\n",
    "    ll = bipartite_likelihood(Lw = Lw, K = K, J = J)\n",
    "    fun = ll + bipartite_prior(nu = nu, Aw = Aw, psi = psi, V = V)\n",
    "    # save measurements of time and objective functions\n",
    "    time_seq.append(time()- start_time)\n",
    "    ll_seq.append(ll)\n",
    "    fun_seq.append(fun)\n",
    "    # compute the relative error and check the tolerance on the Adjacency\n",
    "    # matrix and on the objective function\n",
    "    # check for convergence\n",
    "    werr = abs(w0 - w)\n",
    "    has_w_converged = (np.all(werr <= .5 * reltol * (w + w0)) or np.all(werr <= abstol))\n",
    "    if (has_w_converged):\n",
    "      break\n",
    "    # update estimates\n",
    "    fun0 = fun\n",
    "    w0 = w\n",
    "    V0 = V\n",
    "    psi0 = psi\n",
    "    Aw0 = Aw\n",
    "  results = {\"Laplacian\" : Lw, \"Adjacency\" : Aw, \"obj_fun\" : fun_seq, \"loglike\" : ll_seq, \"w\" : w,\n",
    "                  \"psi\" : psi, \"V\" : V, \"elapsed_time\" : time_seq, \"Lips\" : Lips,\n",
    "                  \"Lips_seq\" : Lips_seq, \"convergence\" : (i < maxiter), \"nu\" : nu}\n",
    "  return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd4a90b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000,loss: 1.9636\n",
      "Epoch: 010,loss: 1.6701\n",
      "Epoch: 020,loss: 1.5129\n",
      "Epoch: 030,loss: 1.4802\n",
      "Epoch: 040,loss: 1.4702\n",
      "Epoch: 050,loss: 1.4386\n",
      "Epoch: 060,loss: 1.4350\n",
      "Epoch: 070,loss: 1.4801\n",
      "Epoch: 080,loss: 1.3835\n",
      "Epoch: 090,loss: 1.3602\n",
      "Epoch: 100,loss: 1.4975\n",
      "Epoch: 110,loss: 1.4141\n",
      "Epoch: 120,loss: 1.4087\n",
      "Epoch: 130,loss: 1.4189\n",
      "Epoch: 140,loss: 1.4103\n",
      "Epoch: 150,loss: 1.4326\n",
      "Epoch: 160,loss: 1.4394\n",
      "Epoch: 170,loss: 1.4801\n",
      "Epoch: 180,loss: 1.4304\n",
      "Epoch: 190,loss: 1.3934\n",
      "Epoch: 200,loss: 1.5546\n",
      "Epoch: 210,loss: 1.4252\n",
      "Epoch: 220,loss: 1.4301\n",
      "Epoch: 230,loss: 1.3860\n",
      "Epoch: 240,loss: 1.4055\n",
      "Epoch: 250,loss: 1.4589\n",
      "Epoch: 260,loss: 1.4243\n",
      "Epoch: 270,loss: 1.4731\n",
      "Epoch: 280,loss: 1.4188\n",
      "Epoch: 290,loss: 1.4140\n",
      "Epoch: 300,loss: 1.3961\n",
      "Epoch: 310,loss: 1.4435\n",
      "Epoch: 320,loss: 1.5021\n",
      "Epoch: 330,loss: 1.4171\n",
      "Epoch: 340,loss: 1.3918\n",
      "Epoch: 350,loss: 1.4646\n",
      "Epoch: 360,loss: 1.4207\n",
      "Epoch: 370,loss: 1.4678\n",
      "Epoch: 380,loss: 1.4472\n",
      "Epoch: 390,loss: 1.5727\n",
      "Epoch: 400,loss: 1.3963\n",
      "Epoch: 410,loss: 1.4386\n",
      "Epoch: 420,loss: 1.4153\n",
      "Epoch: 430,loss: 1.4395\n",
      "Epoch: 440,loss: 1.4703\n",
      "Epoch: 450,loss: 1.5186\n",
      "Epoch: 460,loss: 1.4102\n",
      "Epoch: 470,loss: 1.4306\n",
      "Epoch: 480,loss: 1.4688\n",
      "Epoch: 490,loss: 1.3900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6531365313653137"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "from random import sample\n",
    "NO_OF_NODES = X.shape[0]\n",
    "Wc=adj2\n",
    "Wc=sparse.csr_matrix(Wc)\n",
    "Wc = Wc.tocoo()\n",
    "row = torch.from_numpy(Wc.row).to(torch.long)\n",
    "col = torch.from_numpy(Wc.col).to(torch.long)\n",
    "edge_index_coarsen2 = torch.stack([row, col], dim=0)\n",
    "edge_weight = torch.from_numpy(Wc.data)\n",
    "\n",
    "device = torch.device('cpu')\n",
    "Y = labels\n",
    "labels_coarse=torch.Tensor(Y).type(torch.long)\n",
    "Wc=Wc.toarray()\n",
    "model=Net().to(device)\n",
    "device = torch.device('cpu')\n",
    "lr=0.1\n",
    "decay=0.01\n",
    "features = torch.Tensor(X)\n",
    "try:\n",
    "  X=np.array(features.todense())\n",
    "except:\n",
    "  X = np.array(features)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=decay)\n",
    "p = X.shape[0]\n",
    "x=range(0,int(p*0.9))\n",
    "xx=range(int(p*0.9),p)\n",
    "from datetime import datetime\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(torch.Tensor(X).to(device),edge_index_coarsen2)\n",
    "    loss = F.nll_loss(out[x], labels_coarse[x])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "now1 = datetime.now()\n",
    "losses=[]\n",
    "for epoch in range(500):\n",
    "    loss=train()\n",
    "    losses.append(loss)\n",
    "    if(epoch%10==0):\n",
    "        print(f'Epoch: {epoch:03d},loss: {loss:.4f}')\n",
    "now2 = datetime.now()\n",
    "pred=model(torch.Tensor(X).to(device),edge_index_coarsen2).argmax(dim=1)\n",
    "\n",
    "def train_accuracy():\n",
    "    model.eval()\n",
    "    correct = (pred[xx] == labels_coarse[xx]).sum()\n",
    "    acc = int(correct) /len(xx)\n",
    "    return acc\n",
    "\n",
    "zz=sample(range(0, int(NO_OF_NODES)), NO_OF_NODES)\n",
    "Wc=sparse.csr_matrix(adj2)\n",
    "Wc = Wc.tocoo()\n",
    "row = torch.from_numpy(Wc.row).to(torch.long)\n",
    "col = torch.from_numpy(Wc.col).to(torch.long)\n",
    "edge_index_coarsen = torch.stack([row, col], dim=0)\n",
    "edge_weight = torch.from_numpy(Wc.data)\n",
    "pred=model(torch.Tensor(X),edge_index_coarsen).argmax(dim=1)\n",
    "pred=np.array(pred)\n",
    "correct =(pred[xx]==labels[xx]).sum()\n",
    "acc=int(correct)/len(xx)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25d742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
